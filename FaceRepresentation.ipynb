{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version is 2.1.0 , device name /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "cmd_line_run = False\n",
    "if not cmd_line_run:\n",
    "    %matplotlib inline\n",
    "collab_mode = False\n",
    "\n",
    "if collab_mode and not cmd_line_run:\n",
    "    # set up tensorflow in collab\n",
    "    %tensorflow_version 2.x\n",
    "# imports\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import warnings # This ignore all the warning messages\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from random import randint\n",
    "from os import path\n",
    "import os\n",
    "import time\n",
    "\n",
    "print(\"Tensorflow version is\", tf.__version__, \", device name\", tf.test.gpu_device_name())\n",
    "\n",
    "batch_size = 1024\n",
    "image_size = (64, 64)\n",
    "if not os.path.exists('./embedding'):\n",
    "    os.makedirs('./embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def prepare_embeddings_from_batch(batch, output_type=tf.float32):\n",
    "    imgs, attributes = batch\n",
    "    embeddings = tf.stack(list(attributes.values()))\n",
    "    embeddings = tf.transpose(embeddings)\n",
    "    embeddings = tf.cast(embeddings, dtype=output_type)\n",
    "    return imgs, embeddings\n",
    "\n",
    "def process_image(img, image_shape):\n",
    "    img = tf.cast(img, tf.float32)/127.5-1 # IMPORTANT, image's pixels are in the range <-1, 1>\n",
    "    img = tf.image.resize(img, image_shape)\n",
    "    return img\n",
    "\n",
    "def load_image(filename):\n",
    "    img = tf.io.read_file(filename)\n",
    "    img = tf.image.decode_jpeg(img)\n",
    "    return img\n",
    "\n",
    "def convert_from_output_to_image(images):\n",
    "    return tf.clip_by_value((images+1)/2, 0, 1)\n",
    "\n",
    "def display_image_from_dataset(data):\n",
    "    for batch in data.take(1):\n",
    "        image, attributes = batch\n",
    "        img_ = image\n",
    "        plt.imshow(img_)\n",
    "        print(img_.shape, np.min(img_), np.max(img_))\n",
    "\n",
    "def save_generated_image(settings, epoch):\n",
    "    save_dir = settings.generated_images_path\n",
    "    if not path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    name = path.join(save_dir,\n",
    "                     'img_{}_{}.png'.format(epoch, get_time()))\n",
    "    plt.savefig(name)\n",
    "\n",
    "\n",
    "def show_images(images, epoch, settings, save_images=False, display_images=False):\n",
    "    print(\"image pixels range\", np.min(images), np.max(images), \"std\", np.std(images))\n",
    "    num_of_images = min(10, images.shape[0])\n",
    "    # (x, y=1)\n",
    "    plt.figure(figsize=(num_of_images, 1))\n",
    "    for i in range(num_of_images):\n",
    "        plt.subplot(1, num_of_images, i + 1)\n",
    "        img = images[i, :, :, :].numpy() #\n",
    "        # img = convert_from_output_to_image(img) # images are already converted in gen_step\n",
    "        # img = (img * 127.5 + 127.5).astype(np.uint8)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "    \n",
    "\n",
    "    if save_images:\n",
    "        save_generated_image(settings, epoch)\n",
    "    if display_images and not cmd_line_run:\n",
    "        plt.show()\n",
    "\n",
    "def load_dataset(image_shape, preprocess_images=True, shuffle_size=500, seed=101, split=tfds.Split.TRAIN):\n",
    "    dataset_name = 'celeb_a'\n",
    "    data = tfds.load(dataset_name, split=split)\\\n",
    "               .shuffle(shuffle_size)\n",
    "    # for each image return a tuple (image, attributes), ignore 'landmarks'\n",
    "    if preprocess_images:\n",
    "        data = data\\\n",
    "            .map(lambda x: (process_image(x['image'], image_shape), x['attributes']))\n",
    "    else:\n",
    "        data = data\\\n",
    "            .map(lambda x: (x['image'], x['attributes']))\n",
    "    return data.batch(batch_size)\n",
    "train_data = load_dataset(image_size)\n",
    "test_data = load_dataset(image_size, split=tfds.Split.TEST)\n",
    "\n",
    "train_count = 162770\n",
    "test_count = 19962"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_size = 2\n",
    "strides = 2\n",
    "initial_filters = 16\n",
    "input_shape= (64, 64, 3)\n",
    "outputs = 40\n",
    "embedding_size = 128\n",
    "epochs = 5\n",
    "\n",
    "model_embedding = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(\n",
    "        filters=initial_filters,\n",
    "        kernel_size=kernel_size,\n",
    "        strides=strides,\n",
    "        input_shape=input_shape),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.LeakyReLU(),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(\n",
    "        filters=initial_filters*2,\n",
    "        kernel_size=kernel_size,\n",
    "        strides=strides),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.LeakyReLU(),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(\n",
    "        filters=initial_filters*4,\n",
    "        kernel_size=kernel_size,\n",
    "        strides=strides),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.LeakyReLU(),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(\n",
    "        filters=initial_filters*8,\n",
    "        kernel_size=kernel_size,\n",
    "        strides=strides*2),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(embedding_size)\n",
    "], name='embedding')\n",
    "\n",
    "model_all = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(outputs, activation='sigmoid', input_shape=(embedding_size, ))\n",
    "], name='all')\n",
    "\n",
    "# model_embedding.summary()\n",
    "# model_all.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_and_weights(model, name):\n",
    "    # save model and its weights\n",
    "    model_json = model.to_json()\n",
    "    with open(\"./embedding/{}_structure.json\".format(name), \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    model.save_weights('./embedding/{}_weights.h5'.format(name))\n",
    "    \n",
    "    print('saved model: {}'.format(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(embedding_model, sigmoids_model):\n",
    "    loss_f = tf.keras.losses.BinaryCrossentropy()\n",
    "    optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "    \n",
    "    def test(test_x, test_y):\n",
    "        test_output = sigmoids_model(embedding_model(test_x))\n",
    "        return loss_f(test_output, test_y)\n",
    "    \n",
    "    def train(data, test_data):\n",
    "        val = []\n",
    "        print_every_iter = 5\n",
    "        time_sum = 0.0\n",
    "        test_x, test_y = prepare_embeddings_from_batch(next(iter(test_data)))\n",
    "        for epoch in range(epochs):\n",
    "            iteration = 0\n",
    "            train_loss = 0.0\n",
    "            for batch in data:\n",
    "                s = time.time()\n",
    "                x, y = prepare_embeddings_from_batch(batch)\n",
    "                \n",
    "                @tf.function\n",
    "                def _train():\n",
    "                    with tf.GradientTape() as tape:\n",
    "                        embedding = embedding_model(x)\n",
    "                        output = sigmoids_model(embedding)\n",
    "                        train_loss = loss_f(output, y)\n",
    "\n",
    "                    grads = tape.gradient(train_loss, \n",
    "                                          embedding_model.trainable_variables + sigmoids_model.trainable_variables)\n",
    "                    optimizer.apply_gradients(zip(grads, \n",
    "                                                 embedding_model.trainable_variables + sigmoids_model.trainable_variables))\n",
    "                _train()\n",
    "                e = time.time()\n",
    "                time_sum += e-s\n",
    "                if (iteration+1)%print_every_iter == 0:\n",
    "                    progress = (int((iteration+1)*batch_size*100 / train_count))\n",
    "                    iter_time = (time_sum/print_every_iter)\n",
    "                    print('epoch ', epoch+1,'progress: ',progress,' %, time ',iter_time)\n",
    "                    time_sum = 0.0\n",
    "                iteration+=1\n",
    "                \n",
    "            \n",
    "            test_loss = test(test_x, test_y)\n",
    "            \n",
    "            save_model_and_weights(embedding_model, 'embedding_new')\n",
    "            epoch_res = 'epoch {}/{}, loss {}, test_loss {}'.format(epoch+1, epochs, train_loss, test_loss)\n",
    "            print(epoch_res)\n",
    "            val.append(epoch_res)\n",
    "        return val\n",
    "            \n",
    "    return train\n",
    "        \n",
    "train_f = train_step(model_embedding, model_all)\n",
    "res = train_f(train_data, test_data)\n",
    "\n",
    "with open('representation', 'w') as f:\n",
    "    f.writelines(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_iter = iter(test_data)\n",
    "x, y = prepare_embeddings_from_batch(next(test_iter))\n",
    "output = model_all(model_embedding(x)).numpy()\n",
    "o = np.array([1 if x > .5 else 0 for x in output[0]], dtype=np.int8)\n",
    "print(f1_m(output, y), recall_m(output, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in test_data:\n",
    "    x, y = prepare_embeddings_from_batch(batch)\n",
    "    output = model_all(model_embedding(x)).numpy()\n",
    "    o = np.array([1 if x > .5 else 0 for x in output[0]], dtype=np.int8)\n",
    "    print(f1_m(output, y), recall_m(output, y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
