{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "cmd_line_run = False\n",
    "if not cmd_line_run:\n",
    "    %matplotlib inline\n",
    "collab_mode = False\n",
    "\n",
    "if collab_mode and not cmd_line_run:\n",
    "    # set up tensorflow in collab\n",
    "    %tensorflow_version 2.x\n",
    "# imports\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import warnings # This ignore all the warning messages\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from os import path\n",
    "import os\n",
    "import time\n",
    "\n",
    "print(\"Tensorflow version is\", tf.__version__, \", device name\", tf.test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def allow_memory_growth():\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            # Currently, memory growth needs to be the same across GPUs\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "        except RuntimeError as e:\n",
    "            # Memory growth must be set before GPUs have been initialized\n",
    "            print(e)\n",
    "\n",
    "allow_memory_growth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def get_time():\n",
    "    return time.strftime(\"%d-%m-%Y_%H-%M-%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def process_image(img, image_shape):\n",
    "    img = tf.cast(img, tf.float32)/127.5-1 # IMPORTANT, image's pixels are in the range <-1, 1>\n",
    "    img = tf.image.resize(img, (64, 64))\n",
    "    return img\n",
    "\n",
    "def load_image(filename):\n",
    "    img = tf.io.read_file(filename)\n",
    "    img = tf.image.decode_jpeg(img)\n",
    "    return img\n",
    "\n",
    "def display_image_from_dataset(data):\n",
    "    if cmd_line_run:\n",
    "        return\n",
    "    for batch in data.take(1):\n",
    "        image, attributes = batch\n",
    "        img_ = (image[0]+1)/2\n",
    "        plt.imshow(img_)\n",
    "        print(img_.shape, np.min(img_), np.max(img_))\n",
    "\n",
    "def save_generated_image(settings, epoch):\n",
    "    save_dir = settings.generated_images_path\n",
    "    if not path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    name = path.join(save_dir,\n",
    "                     'img_{}_{}.png'.format(epoch, get_time()))\n",
    "    plt.savefig(name)\n",
    "\n",
    "\n",
    "def show_images(images, epoch, settings, save_images=False, display_images=False):\n",
    "    print(\"image pixels range\", np.min(images), np.max(images), \"std\", np.std(images))\n",
    "    num_of_images = min(10, images.shape[0])\n",
    "    # (x, y=1)\n",
    "    plt.figure(figsize=(num_of_images, 1))\n",
    "    for i in range(num_of_images):\n",
    "        plt.subplot(1, num_of_images, i + 1)\n",
    "        img = images[i, :, :, :].numpy() #\n",
    "        img = (img * 127.5 + 127.5).astype(np.uint8)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "    \n",
    "\n",
    "    if save_images:\n",
    "        save_generated_image(settings, epoch)\n",
    "    if display_images and not cmd_line_run:\n",
    "        plt.show()\n",
    "\n",
    "def load_dataset(dataset_path, image_shape, use_manually_downloaded_dataset=True, preprocess_images=True, shuffle_size=500, seed=101):\n",
    "    if use_manually_downloaded_dataset:\n",
    "        img_path = path.join(dataset_path, '*.jpg')\n",
    "        data = tf.data.Dataset.list_files(img_path, seed=seed)\\\n",
    "                              .shuffle(shuffle_size)\\\n",
    "                              .map(load_image)\n",
    "        # for each image return a tuple (image, attributes), in this case we have no attributes\n",
    "        if preprocess_images:\n",
    "            data = data.map(lambda x: (process_image(x, image_shape), dict()))\n",
    "        else:\n",
    "            data = data.map(lambda x: (x, dict()))\n",
    "            \n",
    "    else:\n",
    "        dataset_name = 'celeb_a'\n",
    "        data = tfds.load(dataset_name, split=tfds.Split.TRAIN)\\\n",
    "                   .shuffle(shuffle_size)\n",
    "        # for each image return a tuple (image, attributes), ignore 'landmarks'\n",
    "        if preprocess_images:\n",
    "            data = data\\\n",
    "                .map(lambda x: (process_image(x['image'], image_shape), x['attributes']))\n",
    "        else:\n",
    "            data = data\\\n",
    "                .map(lambda x: (x['image'], x['attributes']))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class Settings:\n",
    "    def __init__(self, collab_mode):\n",
    "        self.root_local_path = os.getcwd()\n",
    "        self.root_gdrive_path = '/content/drive'\n",
    "        self.gdrive_project_path = 'My Drive/pp/GSN/FaceGenerator'\n",
    "        self.dataset_name = \"celeb_a\"\n",
    "        self.subdataset_dir=\"1000\"\n",
    "        self.dataset_image_size = (28, 28)\n",
    "        self.image_size = (64, 64)\n",
    "        self.image_channels = 3\n",
    "        self.generator_input_shape = (self.image_size[0], self.image_channels)\n",
    "        self.gdrive_mounted = False\n",
    "        self.collab_mode = collab_mode\n",
    "        self.batch_size = 10\n",
    "        self.epochs = 100\n",
    "        self.save_models = False # save models at the end?\n",
    "        self.mount_gdrive()\n",
    "        \n",
    "    @property\n",
    "    def run_name(self):\n",
    "        return 'run_{}'.format(self.subdataset_dir)\n",
    "        # return \"{}_epochs_{}_batch_{}\".format(self.epochs, self.batch_size, self.subdataset_dir)\n",
    "    \n",
    "    @property\n",
    "    def image_shape(self):\n",
    "        return (*self.image_size, self.image_channels)\n",
    "\n",
    "    @property\n",
    "    def download_path(self):\n",
    "        return path.join('.', 'datasets', self.dataset_name)\n",
    "    # def download_path(self):\n",
    "    #     return path.join(self.get_base_path, 'datasets', self.dataset_name)\n",
    "\n",
    "    @property\n",
    "    def dataset_path(self):\n",
    "        return path.join(self.download_path, self.subdataset_dir)\n",
    "\n",
    "    @property\n",
    "    def tensorboard_log_dir(self):\n",
    "        return path.join(self.get_base_path, 'saved_state', self.run_name, 'tensorboard_logs')\n",
    "\n",
    "    @property\n",
    "    def checkpoint_dir(self):\n",
    "        return path.join(self.get_base_path, 'saved_state', self.run_name, \"ckpt\")\n",
    "\n",
    "    @property\n",
    "    def model_save_path(self):\n",
    "        return path.join(self.get_base_path, 'saved_state', self.run_name, 'models')\n",
    "    \n",
    "    @property\n",
    "    def generated_images_path(self):\n",
    "        return path.join(self.get_base_path, 'saved_state', self.run_name, 'generated_images')\n",
    "    \n",
    "    @property\n",
    "    def get_base_path(self):\n",
    "        if self.collab_mode:\n",
    "            return path.join(self.root_gdrive_path, self.gdrive_project_path)\n",
    "        else:\n",
    "            return self.root_local_path\n",
    "\n",
    "    def mount_gdrive(self):\n",
    "        if self.collab_mode:\n",
    "            if cmd_line_run:\n",
    "                print('cmd line run, abort mounting gdrive')\n",
    "                exit(-1)\n",
    "            from google.colab import drive\n",
    "            project_path = path.join(self.root_gdrive_path, self.gdrive_project_path)\n",
    "            self.gdrive_project_path = path.join(self.root_gdrive_path, self.gdrive_project_path)\n",
    "            drive.mount(self.root_gdrive_path)\n",
    "            self.gdrive_mounted = True\n",
    "        \n",
    "            path_with_imports = path.join(self.root_gdrive_path, self.gdrive_project_path)\n",
    "            print(\"Files in path\", path_with_imports)\n",
    "            !ls /content/drive/My\\ Drive/pp/GSN/FaceGenerator\n",
    "            if path_with_imports not in os.sys.path:\n",
    "                os.sys.path.append(path_with_imports)\n",
    "                \n",
    "class DatasetCache:\n",
    "    def __init__(self, use_manually_downloaded_dataset):\n",
    "        self.path = \"\"\n",
    "        self.batch_size = 0\n",
    "        self._data = None\n",
    "        # should it use dataset downloaded manually or the one downloaded by tfds\n",
    "        self.use_manually_downloaded_dataset = use_manually_downloaded_dataset\n",
    "\n",
    "    def load_data(self, settings):\n",
    "        if self._data is not None and self.path == settings.dataset_path:\n",
    "            self.batch_size = settings.batch_size\n",
    "            return self.data\n",
    "        else:\n",
    "            print(\"downloading and loading data\")\n",
    "            if self.use_manually_downloaded_dataset:\n",
    "                self.download_dataset(settings)\n",
    "            self._data = load_dataset(\n",
    "                settings.dataset_path,\n",
    "                settings.image_size,\n",
    "                use_manually_downloaded_dataset=self.use_manually_downloaded_dataset\n",
    "            )\n",
    "            self.data_path = settings.dataset_path\n",
    "            self.batch_size = settings.batch_size\n",
    "            return self.data\n",
    "        \n",
    "    @property\n",
    "    def data(self):\n",
    "        if self._data is None:\n",
    "            return None\n",
    "        else:\n",
    "            return self._data.batch(self.batch_size)\n",
    "\n",
    "    def download_dataset(self, settings):\n",
    "        import dataset_helpers as ds_helpers\n",
    "        '''Downloads data to dataset_path/dataset_name directory'''\n",
    "        print('dataset download path is {}'.format(settings.download_path))\n",
    "        ds_helpers.download_extract('celeba', settings.download_path)\n",
    "        \n",
    "class TensorboardManager():\n",
    "    def __init__(self):\n",
    "        self.log_path = ''\n",
    "        self.train_summary_writer = None\n",
    "        self.test_summary_writer = None\n",
    "        \n",
    "    def initialize(self, settings):\n",
    "        should_be_updated = False\n",
    "        if settings.collab_mode and self.log_path != \"tensorboard_logs\":\n",
    "            should_be_updated = True\n",
    "            self.log_path = \"/content/drive/My Drive/pp/GSN/FaceGenerator/saved_state/run_img_align_celeba/tensorboard_logs\"\n",
    "        elif not settings.collab_mode and self.log_path != settings.tensorboard_log_dir:\n",
    "            should_be_updated = True\n",
    "            self.log_path = settings.tensorboard_log_dir\n",
    "                \n",
    "        if should_be_updated:\n",
    "            self.train_summary_writer = tf.summary.create_file_writer(path.join(self.log_path, 'train'))\n",
    "            self.test_summary_writer = tf.summary.create_file_writer(path.join(self.log_path, 'test'))\n",
    "            print('Initialized tensorboard log dir with path', self.log_path)\n",
    "            self.launch(settings.collab_mode)\n",
    "        \n",
    "    def launch(self, collab_mode):\n",
    "        if collab_mode:\n",
    "            if cmd_line_run:\n",
    "                print('cmd line mode, abort launching tensorboard')\n",
    "                returns\n",
    "            %reload_ext tensorboard\n",
    "            %tensorboard --logdir \"/content/drive/My Drive/pp/GSN/FaceGenerator/saved_state/run_img_align_celeba/tensorboard_logs\"\n",
    "            from tensorboard import notebook\n",
    "            notebook.list() # View open TensorBoard instances\n",
    "        else:\n",
    "            print('open tensorboard with command')\n",
    "            print('tensorboard --logdir {}'.format(self.log_path))\n",
    "            \n",
    "    def save(self, run_type, data, description, timestamp, datatype, step):\n",
    "        '''\n",
    "        run_type: either `train` or `test`\n",
    "        data: represents scalar, images or list of scalars\n",
    "        description: should be of a length of data, i.e. if data is a scalar, description should be a string\n",
    "        datatype: one of 'scalar', 'scalars', 'images'\n",
    "        '''\n",
    "        def _save(writer):\n",
    "            with writer.as_default():\n",
    "                if datatype == 'scalars':\n",
    "                    for value, name in zip(data, description):\n",
    "                        tf.summary.scalar('{}_{}'.format(name, timestamp), value, step=step)\n",
    "                elif datatype == 'scalar':\n",
    "                    tf.summary.scalar('{}_{}'.format(description, timestamp), data, step=step)\n",
    "                elif datatype == 'images':\n",
    "                    tf.summary.image('{}_{}'.format(description, timestamp), data, step=step)\n",
    "                else:\n",
    "                    print('unknown type', datatype)\n",
    "\n",
    "        if run_type == 'train' and self.train_summary_writer:\n",
    "            _save(self.train_summary_writer)\n",
    "        elif run_type == 'test' and self.train_summary_writer:\n",
    "            _save(self.test_summary_writer)\n",
    "        else:\n",
    "            print('unrecognized option `run_type` or selected writer', run_type,'is None')    \n",
    "            \n",
    "class Environment():\n",
    "    def __init__(self, collab_mode):\n",
    "        self.settings = Settings(collab_mode)\n",
    "        self.models = dict()\n",
    "        self.datasetCache = DatasetCache(use_manually_downloaded_dataset=False)\n",
    "        self.checkpointManager = None\n",
    "        self.tensorboard = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test downloading and loading data\n",
    "\n",
    "dataset = DatasetCache(use_manually_downloaded_dataset=False)\n",
    "settings = Settings(collab_mode)\n",
    "if collab_mode:\n",
    "    settings.subdataset_dir = 'img_align_celeba'\n",
    "dataset.load_data(settings)\n",
    "\n",
    "if not collab_mode:\n",
    "    display_image_from_dataset(dataset.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_embeddings_from_batch(batch):\n",
    "    imgs, attributes = batch\n",
    "    embeddings = tf.stack(list(attributes.values()))\n",
    "    embeddings = tf.transpose(embeddings)\n",
    "    return imgs, embeddings\n",
    "    \n",
    "def describe_embedding(orig_attributes, example):\n",
    "    image, embedding_vector = example\n",
    "    # display image\n",
    "    img_ = (image+1)/2\n",
    "    plt.imshow(img_)\n",
    "    # describe image\n",
    "    for key, value in zip(orig_attributes.keys(), embedding_vector):\n",
    "        print(key, ' '*(30-len(key)), value.numpy())\n",
    "\n",
    "def test_creating_embedding(dataset):\n",
    "    for batch in dataset.data.take(1): # take one batch\n",
    "        # get imgs and embeddings from that batch\n",
    "        imgs, embeddings = prepare_embeddings_from_batch(batch)\n",
    "        # describe first image in the batch and display it\n",
    "        describe_embedding(batch[1], (imgs[0], embeddings[0]) )\n",
    "    \n",
    "# imgs, embeddings = prepare_embeddings_from_batch(batch, settings)\n",
    "test_creating_embedding(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_image_from_dataset(dataset.data)\n",
    "if not collab_mode and not cmd_line_run:\n",
    "    !ls \"./datasets/celeb_a/img_align_celeba\" | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noise Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S6YpknpB6a0C",
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "class NoiseGenerator(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, distribution_size):\n",
    "        super().__init__()\n",
    "        self.distribution_size = distribution_size\n",
    "        # self.data_distributions = self.add_weight(shape=(num_classes, distribution_size), trainable=True)\n",
    "        # self.data_distributions = tf.tile(tf.range(0, num_classes, dtype=tf.float32)[:, tf.newaxis], [1, distribution_size])\n",
    "        # TODO:\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # dists = tf.nn.embedding_lookup(self.data_distributions, inputs)\n",
    "        # dists += tf.random.uniform(tf.shape(dists), -0.35, 0.35)\n",
    "        # return dists\n",
    "        # TODO\n",
    "        return tf.random.normal([tf.shape(inputs)[0], self.distribution_size, 3])\n",
    "    \n",
    "    def diverse_distributions_loss(self):\n",
    "        # TODO\n",
    "        return None\n",
    "    \n",
    "    \n",
    "class StackGANNoiseGenerator(tf.keras.layers.Layer):\n",
    "    def __init__(self, distribution_size=100):\n",
    "        super().__init__()\n",
    "        self.distribution_size = distribution_size\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # batch_size x distribution_size\n",
    "        return tf.random.normal([tf.shape(inputs)[0], self.distribution_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1xwRLPZYmJtL",
    "pycharm": {}
   },
   "source": [
    "Loss functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TnfRHsNBmLsd",
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "kl = tf.keras.losses.KLDivergence()\n",
    "\n",
    "def min_max_discriminator_loss(real_out, gen_out):\n",
    "    real_loss = bce(tf.ones_like(real_out), real_out)\n",
    "    gen_loss = bce(tf.zeros_like(gen_out), gen_out)\n",
    "    return real_loss + gen_loss\n",
    "\n",
    "\n",
    "def min_max_generator_loss(gen_out):\n",
    "    return - min_max_discriminator_loss(tf.ones_like(gen_out), gen_out)\n",
    "\n",
    "\n",
    "def w_discriminator_loss(real_out, gen_out):\n",
    "    res = - (tf.reduce_mean(real_out) - tf.reduce_mean(gen_out))\n",
    "    return res\n",
    "\n",
    "\n",
    "def w_generator_loss(gen_out):\n",
    "    return - tf.reduce_mean(gen_out)\n",
    "\n",
    "def kl_generator_loss(real, gen):\n",
    "    return kl(real, gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layers for generator and discriminator from StackGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLU(tf.keras.layers.Layer):\n",
    "    '''\n",
    "    Multiplies first half of the last dimension\n",
    "    by value of the second half of the last dimension.\n",
    "    Layer introduced in https://github.com/hanzhanggit/StackGAN-v2/blob/master/code/model.py#L47\n",
    "    \n",
    "    Returns the same tensor with its last dimension halved and multiplied by sigmoid function activation.\n",
    "    '''\n",
    "    def __init__(self, **kwargs):\n",
    "        super(GLU, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, data):\n",
    "        rank = data.shape.rank\n",
    "        tensor_shape = data.shape.as_list()\n",
    "        last_dim_half = tensor_shape[-1]//2\n",
    "        # automatic slicing is impossible since the first dimension is None (batch size)\n",
    "        \n",
    "\n",
    "        # begins_1, begins_2 = np.zeros((rank,), dtype=np.int32), np.zeros((rank,), dtype=np.int32)\n",
    "        # begins_2[-1] = last_dim_half # set offset for the last dimension\n",
    "\n",
    "        # sizes = [tensor_shape[i] for i in range(rank)]\n",
    "        # sizes[-1] = last_dim_half # get half of elements from the last dimension\n",
    "\n",
    "        # return tf.slice(data, begins_1, sizes) * tf.math.sigmoid(tf.slice(data, begins_2, sizes))\n",
    "        \n",
    "        \n",
    "        if rank == 2:\n",
    "            return data[:, :last_dim_half] * tf.math.sigmoid(data[:, last_dim_half:])\n",
    "        elif rank == 3:\n",
    "            return data[:, :, :last_dim_half] * tf.math.sigmoid(data[:, :, last_dim_half:])\n",
    "        elif rank == 4:\n",
    "            return data[:, :, :, :last_dim_half] * tf.math.sigmoid(data[:, :, :, last_dim_half:])\n",
    "        elif rank == 5:\n",
    "            return data[:, :, :, :, :last_dim_half] * tf.math.sigmoid(data[:, :, :, :, last_dim_half:])\n",
    "\n",
    "class GeneratorResidualLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, filters, index, **kwargs):\n",
    "        super(GeneratorResidualLayer, self).__init__(**kwargs)\n",
    "        \n",
    "        self.block = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(filters*2, 3, 1, 'same', use_bias=False),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            GLU(),\n",
    "            tf.keras.layers.Conv2D(filters, 3, 1, 'same', use_bias=False),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "        ], name='generator_residual_layer_{}'.format(index))\n",
    "\n",
    "    def call(self, data):\n",
    "        residual = data\n",
    "        out = self.block(data)\n",
    "        return tf.keras.layers.add([out, residual])\n",
    "    \n",
    "class GeneratorUpsampleLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, filters, **kwargs):\n",
    "        super(GeneratorUpsampleLayer, self).__init__(**kwargs)\n",
    "        self.block = tf.keras.Sequential([\n",
    "            tf.keras.layers.UpSampling2D(),\n",
    "            tf.keras.layers.Conv2D(filters*2, 3, 1, 'same', use_bias=False),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            GLU()\n",
    "        ], name='generator_upsample_layer')\n",
    "\n",
    "    def call(self, data):\n",
    "        return self.block(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FwrjmA4t6Oxn",
    "pycharm": {}
   },
   "source": [
    "Generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANNetwork(tf.keras.Model):\n",
    "    def __init__(self, model_name=\"Network\", **kwargs):\n",
    "        self.is_conditional = False\n",
    "        if 'is_conditional' in kwargs:\n",
    "            self.is_conditional = kwargs['is_conditional']\n",
    "            # remove that entry from kwargs\n",
    "            # tf raises an exception when a keyword is not from allowed_kwargs set\n",
    "            del kwargs['is_conditional']\n",
    "        super().__init__(name=model_name, **kwargs)\n",
    "        \n",
    "        self.model = None\n",
    "        \n",
    "\n",
    "    def print_layers(self):\n",
    "        print(self.model)\n",
    "        for layer in self.model:\n",
    "            print(layer.name, \":\", layer.input_shape, \"->\", layer.output_shape)\n",
    "\n",
    "    def summary(self):\n",
    "        self.model.summary()\n",
    "        \n",
    "    @property\n",
    "    def output_layer(self):\n",
    "        return self.model.get_layer(index=-1)\n",
    "    \n",
    "    @property\n",
    "    def output_shape(self):\n",
    "        return self.output_layer.output_shape[1:]\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, data, training):\n",
    "        return self.model(data)\n",
    "\n",
    "    def save_model(self, save_path):\n",
    "        if not path.exists(save_path):\n",
    "            os.makedirs(save_path)\n",
    "        filename = path.join(save_path, '{}_{}'.format(self.name, get_time()))\n",
    "        print(\"Saving model\", self.name, \"as\", filename)\n",
    "        self.model.save(filename)\n",
    "        \n",
    "    def compile_model(self, optimizer, loss):\n",
    "        self.model.compile(optimizer=optimizer, loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables from pytorch\n",
    "init_w = 4 # ?\n",
    "init_h = 4 #?\n",
    "gen_init_filters = 64 # how many filters*2 (channels) should be in the first layer of the generator, n_g number in the paper\n",
    "df_dim = 64\n",
    "gf_dim=gen_init_filters # ?\n",
    "z_dim=(100, 100) # size of the vector with normal distribution noise\n",
    "embeddings_shape = (100, 40)\n",
    "n_g = gen_init_filters\n",
    "n_d = 64\n",
    "batch_size=24\n",
    "settings.batch_size = batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class GeneratorInitStage(GANNetwork):\n",
    "    #input_shape = cfg.GAN.Z_DIM + cfg.GAN.EMBEDDING_DIM\n",
    "    # or \n",
    "    # input_shape = cfg.GAN.Z_DIM if without conditioning\n",
    "    def __init__(self, noise_shape, embeddings_shape, filters, model_name=\"GeneratorInitStage\", **kwargs):\n",
    "        # filters should be `gen_init_filters`\n",
    "        super().__init__(model_name, **kwargs)\n",
    "        self.initial_factor = 16\n",
    "        self.filters = self.initial_factor*filters\n",
    "        # if is conditional then the second dimension is has size of concatenated noise and embedding\n",
    "        \n",
    "        self.in_shape = \\\n",
    "            (noise_shape[1] + embeddings_shape[1]) if self.is_conditional else noise_shape[1]\n",
    "#         print('in shape', self.in_shape)\n",
    "        \n",
    "        self.model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(init_w*init_h*self.filters*2, input_shape=(self.in_shape, )),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            GLU(),\n",
    "            tf.keras.layers.Reshape([init_w, init_h, self.filters]),\n",
    "            GeneratorUpsampleLayer(self.filters//2),\n",
    "            GeneratorUpsampleLayer(self.filters//4),\n",
    "            GeneratorUpsampleLayer(self.filters//8),\n",
    "            GeneratorUpsampleLayer(self.filters//16)\n",
    "        ], name='generator_intermediate_init_output')\n",
    "        \n",
    "        self.output_model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(3, 3, 1, 'same', input_shape=self.output_shape)\n",
    "        ], name = 'generator_init_stage_output')\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, data, training):\n",
    "        intermediate_output = self.model(data)\n",
    "        output = self.output_model(intermediate_output)\n",
    "        return intermediate_output, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorNextStage(GANNetwork):\n",
    "    @staticmethod\n",
    "    def prepare_embedding_tensor(embedding, output_shape):\n",
    "        embedding_shape = embedding.shape.as_list()\n",
    "        # multiply output_shape[0] * output_shape[1]\n",
    "        repeat_factor = tf.math.reduce_prod(output_shape)\n",
    "        res = tf.keras.backend.repeat(embedding, repeat_factor)\n",
    "        reshape_s = [embedding_shape[0], *output_shape, embedding_shape[-1]]\n",
    "        print(res.shape.as_list(), reshape_s)\n",
    "        # reshape to [batch_size, output_shape[0], output_shape[1], embedding_size]\n",
    "        return tf.reshape(res, reshape_s)\n",
    "    \n",
    "    @staticmethod\n",
    "    def concat_embedding_and_previous_output(embedding, previous_output):\n",
    "        repeat_factor = 1\n",
    "#         print(embedding.shape.as_list(), previous_output.shape.as_list())\n",
    "        if previous_output.shape.rank == 4:\n",
    "            # get second and third dimension\n",
    "            previous_output_shape = tf.slice(previous_output.shape, begin=[1], size=[2]).numpy()\n",
    "        else:\n",
    "            raise(\"Rank of the previous_output is not equal to 4\")\n",
    "        embedding = GeneratorNextStage.prepare_embedding_tensor(embedding, previous_output_shape)\n",
    "        print('embedding', embedding.shape.as_list())\n",
    "        return tf.concat([embedding, previous_output], axis=-1)\n",
    "    \n",
    "\n",
    "    def __init__(self, embeddings_shape, number_of_residuals, previous_stage_shape, filters, model_name=\"GeneratorInitStage\", **kwargs):\n",
    "        super().__init__(model_name, **kwargs)\n",
    "        \n",
    "        # size should be equal to\n",
    "        # (init_stage_output_y, init_stage_output_x, init_stage_output_channels\n",
    "        self.in_shape = previous_stage_shape\n",
    "        if self.is_conditional:\n",
    "            # concat embeddings with channel\n",
    "            self.in_shape = (self.in_shape[0], self.in_shape[1], self.in_shape[2] + embeddings_shape[1])\n",
    "        \n",
    "        self.model = tf.keras.Sequential()\n",
    "        \n",
    "        self.model.add(tf.keras.layers.Dense(filters*2, input_shape=self.in_shape))\n",
    "        self.model.add(tf.keras.layers.BatchNormalization())\n",
    "        self.model.add(GLU())\n",
    "        \n",
    "        self.model.add(GeneratorResidualLayer(filters, index=0))\n",
    "        for i in range(number_of_residuals-1):\n",
    "            self.model.add(GeneratorResidualLayer(filters, i+1))\n",
    "        \n",
    "        self.model.add(GeneratorUpsampleLayer(filters//2))\n",
    "        \n",
    "        self.output_model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(3, 3, 1, 'same', input_shape=self.output_shape)\n",
    "        ], name = 'generator_next_stage_output')\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, data, training):\n",
    "        intermediate_output = self.model(data)\n",
    "        output = self.output_model(intermediate_output)\n",
    "        return intermediate_output, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intermediate out [10 64 64 64]\n",
      "[10, 4096, 40] [10, 64, 64, 40]\n",
      "embedding [10, 64, 64, 40]\n"
     ]
    }
   ],
   "source": [
    "# test init and next stages\n",
    "init_stage = GeneratorInitStage(noise_shape=z_dim,\n",
    "                                embeddings_shape=embeddings_shape,\n",
    "                                filters=gen_init_filters,\n",
    "                                is_conditional=True)\n",
    "# init_stage.summary()\n",
    "# init_stage.output_model.summary()\n",
    "\n",
    "h_code_shape = init_stage.output_shape # shape of the output of the INIT_STAGE_G intermediate_output layer\n",
    "\n",
    "first_next_stage = GeneratorNextStage(\n",
    "    embeddings_shape=embeddings_shape,\n",
    "    number_of_residuals=2,\n",
    "    previous_stage_shape=h_code_shape,\n",
    "    filters=gf_dim,\n",
    "    is_conditional=True)\n",
    "# first_next_stage.summary()\n",
    "# first_next_stage.output_model.summary()\n",
    "\n",
    "# second_next_stage = GeneratorNextStage(\n",
    "#     embeddings_shape=embeddings_shape,\n",
    "#     number_of_residuals=2,\n",
    "#     previous_stage_shape=first_next_stage.output_shape,\n",
    "#     filters=gf_dim,\n",
    "#     is_conditional=True)\n",
    "\n",
    "# second_next_stage.summary()\n",
    "# second_next_stage.output_model.summary()\n",
    "\n",
    "noise_generator = StackGANNoiseGenerator()\n",
    "for batch in dataset.data.take(1):\n",
    "    imgs, embedding = prepare_embeddings_from_batch(batch)\n",
    "    emb = tf.cast(embedding, dtype=tf.float32)\n",
    "    \n",
    "    noise = noise_generator(imgs)\n",
    "    \n",
    "    embeddings = tf.concat([emb, noise], axis=1)\n",
    "\n",
    "    intermediate_out, init_out = init_stage(embeddings, training=False)\n",
    "    print('intermediate out', tf.shape(intermediate_out).numpy())\n",
    "    next_input = GeneratorNextStage.concat_embedding_and_previous_output(embedding=emb, previous_output=intermediate_out)\n",
    "    out_next = first_next_stage(next_input, training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(GANNetwork):\n",
    "    def __init__(self, input_shape, model_name=\"Generator\", **kwargs):\n",
    "        super().__init__(model_name, **kwargs)\n",
    "        self.model = tf.keras.Sequential([\n",
    "            # flat\n",
    "            tf.keras.layers.Dense(1024, input_shape=input_shape),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Reshape([16, 16, 256]),\n",
    "            # conv without stride (16x16)\n",
    "            tf.keras.layers.Conv2D(256, 5, 1, 'same'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            # t_conv with stride (32x32)\n",
    "            tf.keras.layers.Conv2DTranspose(128, 5, 2, 'same'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            # conv without stride (32x32)\n",
    "            tf.keras.layers.Conv2D(64, 5, 1, 'same'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            # t_conv with stride (64x64)\n",
    "            tf.keras.layers.Conv2DTranspose(32, 5, 2, 'same'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            # conv without stride\n",
    "            tf.keras.layers.Conv2D(3, (1, 1), 1, 'same')\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8N1JmZB06SKO",
    "pycharm": {}
   },
   "source": [
    "Discriminator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscriminatorEncodeImageLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, filters, **kwargs):\n",
    "        super(DiscriminatorEncodeImageLayer, self).__init__(**kwargs)\n",
    "        self.block = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(filters, 4, 2, 'same', use_bias=False),\n",
    "            tf.keras.layers.LeakyReLU(alpha=.2),\n",
    "            \n",
    "            tf.keras.layers.Conv2D(filters*2, 4, 2, 'same', use_bias=False),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.LeakyReLU(alpha=.2),\n",
    "            \n",
    "            tf.keras.layers.Conv2D(filters*4, 4, 2, 'same', use_bias=False),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.LeakyReLU(alpha=.2),\n",
    "            \n",
    "            \n",
    "            tf.keras.layers.Conv2D(filters*8, 4, 2, 'same', use_bias=False),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.LeakyReLU(alpha=.2),\n",
    "        ], name='discriminator_encoding_layer')\n",
    "\n",
    "    def call(self, data):\n",
    "        return self.block(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackGANDiscriminator(GANNetwork):\n",
    "    def __init__(self, input_shape, embedding_shape, filters, model_name=\"StackGANDiscriminator\", **kwargs):\n",
    "        super().__init__(model_name, **kwargs)\n",
    "        \n",
    "        self.filters = filters\n",
    "        self.model = tf.keras.Sequential()\n",
    "        self.model.add(DiscriminatorEncodeImageLayer(filters = self.filters))\n",
    "        \n",
    "        if self.is_conditional:\n",
    "            self.model.add(tf.keras.layers.Reshape()\n",
    "        \n",
    "discriminator_64 = StackGANDiscriminator(input_shape=(64, 64, 3), filters=df_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "16ATdVrY6UId",
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "class Discriminator(GANNetwork):\n",
    "\n",
    "    def __init__(self, input_shape, model_name=\"Discriminator\", **kwargs):\n",
    "        super().__init__(model_name, **kwargs)\n",
    "\n",
    "        # since discriminator is for classification it should be robust, thus, add\n",
    "        # additional regularization like dropout to prevent from pixel attacks\n",
    "        self.model = tf.keras.Sequential([\n",
    "            # conv with stride (32x32)\n",
    "            tf.keras.layers.Conv2D(64, 5, 2, 'same', input_shape=input_shape),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            # conv with stride (16x16x128)\n",
    "            tf.keras.layers.Conv2D(128, 3, 2, 'same'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            # flatten + hidden layer\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(64),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            # prediction (LOGITS!)\n",
    "            tf.keras.layers.Dense(1)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(input_shape=settings.generator_input_shape)\n",
    "generator.build((None, *settings.generator_input_shape))\n",
    "generator.summary()\n",
    "\n",
    "discriminator = Discriminator(input_shape=settings.image_shape)\n",
    "discriminator.build(input_shape=(None, *settings.image_shape))\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First implementation, no stack gan\n",
    "Generator:\n",
    "\n",
    "   * Total params: 2,724,291\n",
    "   * Trainable params: 2,721,283\n",
    "   * Non-trainable params: 3,008\n",
    "        \n",
    "Discriminator:\n",
    "\n",
    "   * Total params: 2,177,025\n",
    "   * Trainable params: 2,176,513\n",
    "   * Non-trainable params: 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "enYw9PQ_6h_o",
    "pycharm": {}
   },
   "source": [
    "Training step\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OqIPgp5B6jqe",
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "def train_step_template(generator, discriminator, noise, d_optim, g_optim, d_loss_f, g_loss_f, kl_loss_f):\n",
    "\n",
    "    @tf.function\n",
    "    def _train_step_template(images, epoch):\n",
    "        with tf.GradientTape() as d_tape, tf.GradientTape() as g_tape:\n",
    "            real_out = discriminator(images, True)\n",
    "            generated = generator(noise(images), True)\n",
    "            gen_out = discriminator(generated, True)\n",
    "\n",
    "            d_loss = d_loss_f(real_out, gen_out)\n",
    "            g_loss = g_loss_f(gen_out)\n",
    "            g_kl_loss = kl_loss_f(images, generated)\n",
    "\n",
    "        d_grads = d_tape.gradient(d_loss, discriminator.trainable_variables)\n",
    "        g_grads = g_tape.gradient(g_loss, generator.trainable_variables + noise.trainable_variables)\n",
    "\n",
    "        d_optim.apply_gradients(zip(d_grads, discriminator.trainable_variables))\n",
    "        g_optim.apply_gradients(zip(g_grads, generator.trainable_variables + noise.trainable_variables))\n",
    "        \n",
    "        return d_loss, g_loss, g_kl_loss\n",
    "\n",
    "    return _train_step_template\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "axR-vkyz6kZa",
    "pycharm": {}
   },
   "source": [
    "Inference step\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r57i63lv6mRT",
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "def gen_step_template(generator, noise):\n",
    "    @tf.function\n",
    "    def _gen_step_template(images):\n",
    "        return tf.clip_by_value(generator(noise(images), False), -1, 1)\n",
    "\n",
    "    return _gen_step_template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8xRLPJoW7IcI",
    "pycharm": {}
   },
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ut1yWYzD7JeR",
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "def train(train_step, gen_step, epochs, data, settings, ckptManager, tensorboard_manager, save_images=True, display_images=True):\n",
    "    loss_descriptions = ['discriminator loss', 'generator loss', 'kl generator loss']\n",
    "    timestamp = get_time()\n",
    "    max_batch_iter = 0\n",
    "    for epoch in range(epochs):\n",
    "        batch_iter = 0\n",
    "        epoch_start = time.time()\n",
    "        for images_and_attributes in data:\n",
    "            images = images_and_attributes[0]\n",
    "            batch_iter+=1\n",
    "            train_result = train_step(images, epoch)\n",
    "            tensorboard_manager.save('train',\n",
    "                                     [np.average(loss) for loss in train_result],\n",
    "                                     loss_descriptions,\n",
    "                                     timestamp,\n",
    "                                     'scalars',\n",
    "                                     epoch*max_batch_iter+batch_iter\n",
    "                                    )\n",
    "            if (batch_iter+1) % 50 == 0:\n",
    "                print(\"Saving checkpoint, iter\", batch_iter+1)\n",
    "                ckptManager.save()\n",
    "        # save the number of batches in one epoch\n",
    "        if epoch == 0:\n",
    "            max_batch_iter = batch_iter\n",
    "\n",
    "        epoch_end = time.time()\n",
    "        print('-'*30)\n",
    "        print('Epoch {0}/{1}, duration {2}'.format(epoch+1, epochs, epoch_end-epoch_start))\n",
    "        #if (epoch + 1) % 5 == 0 or epoch == epochs-1:\n",
    "        print(\"Saving checkpoint, epoch\", epoch+1)\n",
    "        ckptManager.save()\n",
    "\n",
    "        images_to_generate = [img[0] for img in data.take(1)][0].numpy() # take one batch from train_data\n",
    "        generated = gen_step(images_to_generate)\n",
    "        \n",
    "        tensorboard_manager.save('train',\n",
    "                                 generated,\n",
    "                                 '{}_{}/{}'.format(settings.run_name, epoch+1, epochs),\n",
    "                                 timestamp,\n",
    "                                 'images',\n",
    "                                 epoch\n",
    "                                )\n",
    "            \n",
    "        show_images(generated, epoch, settings, save_images=save_images, display_images=display_images)\n",
    "        print('+'*30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how images are displayed/saved\n",
    "def test_image_generation(settings):\n",
    "    save_images = True\n",
    "    display_images = True\n",
    "    batch_size = 10\n",
    "    data = load_dataset(batch_size=settings.batch_size)\n",
    "    \n",
    "    generator_input_shape = (64, 3)\n",
    "    generator = Generator(input_shape=generator_input_shape)\n",
    "    noise = NoiseGenerator(64)\n",
    "    \n",
    "    gen_step = gen_step_template(\n",
    "        generator=generator,\n",
    "        noise=noise\n",
    "    )\n",
    "    images_to_generate = [img for img in data.take(1)][0].numpy() # take one batch from train_data\n",
    "    generated = gen_step(images_to_generate)\n",
    "    show_images(generated, -1, settings, save_images=save_images, display_images=display_images)\n",
    "    \n",
    "# test_image_generation(Settings(collab_mode))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i1EZ3WMg7ZS0",
    "pycharm": {}
   },
   "source": [
    "Training with Wasserstein loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316
    },
    "colab_type": "code",
    "id": "cxqxoONqA4ZR",
    "outputId": "c235a412-fe41-40b4-fa73-673d8b55560e",
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "def get_models(settings):\n",
    "    generator = Generator(input_shape=settings.generator_input_shape)\n",
    "    generator.build((None, *settings.generator_input_shape))\n",
    "#     generator.summary()\n",
    "\n",
    "    discriminator = Discriminator(input_shape=settings.image_shape)\n",
    "    discriminator.build(input_shape=(None, *settings.image_shape))\n",
    "#     discriminator.summary()\n",
    "    \n",
    "    return generator, discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "EHKejxSy7YOx",
    "outputId": "de999bc7-5064-4d75-86e6-487f676dce28",
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "env = Environment(collab_mode)\n",
    "env.settings.epochs = 30\n",
    "env.settings.batch_size = 500\n",
    "env.settings.subdataset_dir='img_align_celeba'\n",
    "env.settings.save_models = True\n",
    "print(env.settings.get_base_path)\n",
    "# load data\n",
    "env.datasetCache.load_data(env.settings)\n",
    "# set models\n",
    "env.models['generator'], env.models['discriminator'] = get_models(env.settings)\n",
    "# setup tensorboard\n",
    "env.tensorboard = TensorboardManager()\n",
    "env.tensorboard.initialize(env.settings)\n",
    "\n",
    "noise = StackGANNoiseGenerator(100)\n",
    "d_optim = tf.keras.optimizers.Adam(1e-4)\n",
    "g_optim = tf.keras.optimizers.Adam(1e-4)\n",
    "\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=g_optim,\n",
    "                                 discriminator_optimizer=d_optim,\n",
    "                                 generator=env.models['generator'],\n",
    "                                 discriminator=env.models['discriminator'])\n",
    "env.checkpointManager = tf.train.CheckpointManager(checkpoint=checkpoint,\n",
    "                                                   directory=env.settings.checkpoint_dir,\n",
    "                                                   max_to_keep=3\n",
    "                                                  )\n",
    "if env.checkpointManager.latest_checkpoint:\n",
    "    print(\"restoring state from\", env.checkpointManager.latest_checkpoint)\n",
    "    checkpoint\\\n",
    "        .restore(env.checkpointManager.latest_checkpoint)\n",
    "    \n",
    "train_step = train_step_template(\n",
    "    generator=env.models['generator'],\n",
    "    discriminator=env.models['discriminator'],\n",
    "    noise=noise,\n",
    "    d_optim=d_optim,\n",
    "    g_optim=g_optim,\n",
    "    d_loss_f=w_discriminator_loss,\n",
    "    g_loss_f=w_generator_loss,\n",
    "    kl_loss_f=kl_generator_loss\n",
    ")\n",
    "\n",
    "gen_step = gen_step_template(\n",
    "    generator=env.models['generator'],\n",
    "    noise=noise\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Start time\", get_time())\n",
    "print('%'*30)\n",
    "start = time.time()\n",
    "\n",
    "train(\n",
    "    train_step=train_step,\n",
    "    gen_step=gen_step,\n",
    "    epochs=env.settings.epochs,\n",
    "    data=env.datasetCache.data,\n",
    "    settings=env.settings,\n",
    "    ckptManager=env.checkpointManager,\n",
    "    tensorboard_manager=env.tensorboard\n",
    ")\n",
    "\n",
    "end = time.time()\n",
    "print('%'*30)\n",
    "print(\"End time\", get_time())\n",
    "print(\"seconds elapsed\", end - start)\n",
    "\n",
    "if env.settings.save_models:\n",
    "    print('saving models')\n",
    "    for model in env.models:\n",
    "        env.models[model].save_model(env.settings.model_save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "uwagi:\n",
    "* na początku generowane obrazki są białe, bardzo małe odchylenie w wartościach pikseli ok 17 dla skali 0-255\n",
    "* generator używa tylko skali np 52-160\n",
    "* później generator uczy się zwiększać odchylenie i wartości pikseli na obrazkach zwiększają się do przedziału 0-255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "FaceGenerator",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
